#!/usr/bin/env python3
"""
MCP Server with Gemini Integration
Provides development workflow automation with AI second opinions
"""
import asyncio
import json
import os
import sys
from pathlib import Path
from typing import Any, Dict, List

import mcp.server.stdio
import mcp.types as types
from mcp.server import Server

# Import Gemini integration
from gemini_integration import get_integration


class MCPServer:
    def __init__(self, project_root: str = None):
        self.project_root = Path(project_root) if project_root else Path.cwd()
        self.server = Server("gemini-mcp-integration")
        
        # Initialize Gemini integration with singleton pattern
        self.gemini_config = self._load_gemini_config()
        # Get the singleton instance, passing config on first call
        self.gemini = get_integration(self.gemini_config)
        
        self._setup_tools()
        
        print(f"MCP Server initialized with project root: {self.project_root}")
        print(f"Gemini integration enabled: {self.gemini.enabled}")

    def _load_gemini_config(self) -> Dict[str, Any]:
        """Load Gemini configuration from file and environment"""
        config = {}
        
        # Load from config file if exists
        config_file = self.project_root / "gemini-config.json"
        if config_file.exists():
            try:
                with open(config_file) as f:
                    config = json.load(f)
                print(f"Loaded configuration from {config_file}")
            except Exception as e:
                print(f"Warning: Failed to load config file {config_file}: {e}")
                config = {}
        else:
            print(f"No config file found at {config_file}, using defaults")
        
        # Override with environment variables
        env_mapping = {
            'GEMINI_ENABLED': ('enabled', lambda x: x.lower() == 'true'),
            'GEMINI_AUTO_CONSULT': ('auto_consult', lambda x: x.lower() == 'true'),
            'GEMINI_CLI_COMMAND': ('cli_command', str),
            'GEMINI_TIMEOUT': ('timeout', int),
            'GEMINI_RATE_LIMIT': ('rate_limit_delay', float),
            'GEMINI_MODEL': ('model', str),
            'GEMINI_MAX_CONTEXT': ('max_context_length', int),
        }
        
        env_overrides = 0
        for env_key, (config_key, converter) in env_mapping.items():
            value = os.getenv(env_key)
            if value is not None:
                try:
                    config[config_key] = converter(value)
                    env_overrides += 1
                except ValueError as e:
                    print(f"Warning: Invalid value for {env_key}: {value} ({e})")
        
        if env_overrides > 0:
            print(f"Applied {env_overrides} environment variable overrides")
        
        return config

    def _setup_tools(self):
        """Register all MCP tools"""
        @self.server.list_tools()
        async def handle_list_tools():
            return [
                types.Tool(
                    name="consult_gemini",
                    description="Consult Gemini for a second opinion or validation",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "query": {
                                "type": "string",
                                "description": "The question or topic to consult Gemini about"
                            },
                            "context": {
                                "type": "string",
                                "description": "Additional context for the consultation"
                            },
                            "comparison_mode": {
                                "type": "boolean",
                                "description": "Whether to request structured comparison format",
                                "default": True
                            }
                        },
                        "required": ["query"]
                    }
                ),
                types.Tool(
                    name="gemini_status",
                    description="Check Gemini integration status and statistics",
                    inputSchema={
                        "type": "object",
                        "properties": {},
                        "required": []
                    }
                ),
                types.Tool(
                    name="toggle_gemini_auto_consult",
                    description="Enable or disable automatic Gemini consultation",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "enable": {
                                "type": "boolean",
                                "description": "Enable (true) or disable (false) auto-consultation"
                            }
                        },
                        "required": []
                    }
                ),
                types.Tool(
                    name="enhance_request",
                    description="ì‚¬ìš©ì ìš”ì²­ì„ Geminië¡œ ë¶„ì„í•˜ê³  êµ¬ì²´ì ì¸ ìš”êµ¬ì‚¬í•­ìœ¼ë¡œ ê°œì„ ",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "user_request": {
                                "type": "string",
                                "description": "ì‚¬ìš©ìì˜ ê°„ë‹¨í•œ ìš”ì²­ (ì˜ˆ: 'ë‚´ ì•±ì— êµ¬ê¸€ ë¡œê·¸ì¸ ê¸°ëŠ¥ ë¶™ì´ê³  ì‹¶ì–´')"
                            },
                            "project_context": {
                                "type": "string",
                                "description": "í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ (ê¸°ìˆ  ìŠ¤íƒ, í˜„ì¬ ìƒíƒœ ë“±)"
                            }
                        },
                        "required": ["user_request"]
                    }
                ),
                types.Tool(
                    name="smart_code_generation",
                    description="ê°œì„ ëœ ìš”ì²­ìœ¼ë¡œ ë‹¨ê³„ë³„ ì½”ë“œ ìƒì„± ê°€ì´ë“œ ì œê³µ",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "enhanced_request": {
                                "type": "string",
                                "description": "enhance_requestë¡œ ê°œì„ ëœ ìƒì„¸ ìš”êµ¬ì‚¬í•­"
                            },
                            "tech_stack": {
                                "type": "string",
                                "description": "ì‚¬ìš©í•  ê¸°ìˆ  ìŠ¤íƒ (ì˜ˆ: React, Node.js, Python Django ë“±)"
                            },
                            "complexity_level": {
                                "type": "string",
                                "description": "êµ¬í˜„ ë³µì¡ë„ (basic, intermediate, advanced)",
                                "default": "intermediate"
                            }
                        },
                        "required": ["enhanced_request"]
                    }
                ),
                types.Tool(
                    name="enhance_user_request",
                    description="ì‚¬ìš©ì ìš”ì²­ì„ í•œ ë²ˆì— ë¶„ì„í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ê°œë°œ ê³„íšìœ¼ë¡œ ë³€í™˜",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "user_request": {
                                "type": "string",
                                "description": "ì‚¬ìš©ìì˜ ê°„ë‹¨í•œ ê°œë°œ ìš”ì²­"
                            },
                            "project_info": {
                                "type": "string",
                                "description": "í”„ë¡œì íŠ¸ ì •ë³´ (ê¸°ìˆ  ìŠ¤íƒ, í˜„ì¬ ìƒíƒœ, ì œì•½ì‚¬í•­ ë“±)"
                            },
                            "output_format": {
                                "type": "string",
                                "description": "ì¶œë ¥ í˜•ì‹ (detailed_plan, quick_guide, step_by_step)",
                                "default": "detailed_plan"
                            }
                        },
                        "required": ["user_request"]
                    }
                )
            ]
        
        @self.server.call_tool()
        async def handle_call_tool(name: str, arguments: Dict[str, Any]):
            if name == "consult_gemini":
                return await self._handle_consult_gemini(arguments)
            elif name == "gemini_status":
                return await self._handle_gemini_status(arguments)
            elif name == "toggle_gemini_auto_consult":
                return await self._handle_toggle_auto_consult(arguments)
            elif name == "enhance_request":
                return await self._handle_enhance_request(arguments)
            elif name == "smart_code_generation":
                return await self._handle_smart_code_generation(arguments)
            elif name == "enhance_user_request":
                return await self._handle_enhance_user_request(arguments)
            else:
                raise ValueError(f"Unknown tool: {name}")

    async def _handle_consult_gemini(self, arguments: Dict[str, Any]) -> List[types.TextContent]:
        """Handle Gemini consultation requests"""
        query = arguments.get('query', '')
        context = arguments.get('context', '')
        comparison_mode = arguments.get('comparison_mode', True)
        
        if not query:
            return [types.TextContent(
                type="text",
                text="âŒ Error: 'query' parameter is required for Gemini consultation"
            )]
        
        print(f"Processing Gemini consultation request: {query[:50]}...")
        
        result = await self.gemini.consult_gemini(
            query=query,
            context=context,
            comparison_mode=comparison_mode
        )
        
        if result['status'] == 'success':
            response_text = f"ğŸ¤– **Gemini Second Opinion**\n\n{result['response']}\n\n"
            response_text += f"â±ï¸ *Consultation completed in {result['execution_time']:.2f}s*"
            response_text += f"\nğŸ“‹ *Consultation ID: {result['consultation_id']}*"
        elif result['status'] == 'disabled':
            response_text = "âš ï¸ **Gemini Integration Disabled**\n\nGemini integration is currently disabled. Enable it with the toggle_gemini_auto_consult tool."
        else:
            error_suggestion = self.gemini.get_error_suggestion(result.get('error_type', 'unknown'))
            response_text = f"âŒ **Gemini Consultation Failed**\n\n"
            response_text += f"**Error:** {result.get('error', 'Unknown error')}\n\n"
            response_text += f"**Suggestion:** {error_suggestion}"
        
        return [types.TextContent(type="text", text=response_text)]

    async def _handle_gemini_status(self, arguments: Dict[str, Any]) -> List[types.TextContent]:
        """Handle Gemini status requests"""
        status_info = self.gemini.get_status_info()
        
        status_lines = [
            "ğŸ¤– **Gemini Integration Status**",
            "",
            f"â€¢ **Enabled**: {'âœ… Yes' if status_info['enabled'] else 'âŒ No'}",
            f"â€¢ **Auto-consult**: {'âœ… Yes' if status_info['auto_consult'] else 'âŒ No'}",
            f"â€¢ **CLI Command**: `{status_info['cli_command']}`",
            f"â€¢ **Model**: {status_info['model']}",
            f"â€¢ **Rate Limit**: {status_info['rate_limit_delay']}s between calls",
            f"â€¢ **Timeout**: {status_info['timeout']}s",
            f"â€¢ **Max Context**: {status_info['max_context_length']} characters",
            "",
            f"ğŸ“Š **Statistics**:",
            f"â€¢ **Total Consultations**: {status_info['total_consultations']}",
            f"â€¢ **Successful**: {status_info['successful_consultations']}",
            f"â€¢ **Failed**: {status_info['failed_consultations']}",
        ]
        
        if status_info['last_consultation']:
            status_lines.append(f"â€¢ **Last Consultation**: {status_info['last_consultation']}")
        
        return [types.TextContent(type="text", text="\n".join(status_lines))]
    
    async def _handle_toggle_auto_consult(self, arguments: Dict[str, Any]) -> List[types.TextContent]:
        """Handle toggle auto-consultation requests"""
        enable = arguments.get('enable')
        
        if enable is None:
            # Toggle current state
            self.gemini.auto_consult = not self.gemini.auto_consult
        else:
            self.gemini.auto_consult = enable
        
        status = "enabled" if self.gemini.auto_consult else "disabled"
        response_text = f"ğŸ”„ **Auto-consultation {status}**\n\n"
        
        if self.gemini.auto_consult:
            response_text += "Gemini will now be automatically consulted when uncertainty patterns are detected."
        else:
            response_text += "Automatic consultation disabled. Use consult_gemini tool for manual consultations."
        
        print(f"Auto-consultation {status}")
        
        return [types.TextContent(
            type="text",
            text=response_text
        )]

    async def _handle_enhance_request(self, arguments: Dict[str, Any]) -> List[types.TextContent]:
        """ì‚¬ìš©ì ìš”ì²­ì„ Geminië¡œ ë¶„ì„í•˜ê³  êµ¬ì²´ì ì¸ ìš”êµ¬ì‚¬í•­ìœ¼ë¡œ ê°œì„ """
        user_request = arguments.get('user_request', '')
        project_context = arguments.get('project_context', '')
        
        if not user_request:
            return [types.TextContent(
                type="text",
                text="âŒ Error: 'user_request' parameter is required"
            )]
        
        # Geminiì—ê²Œ ìš”ì²­ ê°œì„ ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        enhancement_prompt = f"""
ì‚¬ìš©ìì˜ ê°„ë‹¨í•œ ê°œë°œ ìš”ì²­ì„ ë¶„ì„í•˜ê³  êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ìš”êµ¬ì‚¬í•­ìœ¼ë¡œ í™•ì¥í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ìš”ì²­: "{user_request}"
í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸: {project_context if project_context else "ì •ë³´ ì—†ìŒ"}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

ğŸ“‹ **êµ¬ì²´ì  ìš”êµ¬ì‚¬í•­:**
- í•µì‹¬ ê¸°ëŠ¥ë“¤ì„ ëª…í™•íˆ ë‚˜ì—´
- ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­ í¬í•¨
- ë³´ì•ˆ ë° ì—ëŸ¬ ì²˜ë¦¬ ê³ ë ¤

ğŸ”§ **ê¸°ìˆ  ìŠ¤íƒ ê³ ë ¤ì‚¬í•­:**
- ê¶Œì¥ ë¼ì´ë¸ŒëŸ¬ë¦¬/í”„ë ˆì„ì›Œí¬
- ì„¤ì • ë° í™˜ê²½ ìš”êµ¬ì‚¬í•­

ğŸ“ **êµ¬í˜„ ìˆœì„œ:**
1. ë‹¨ê³„ë³„ êµ¬í˜„ ê³„íš
2. ìš°ì„ ìˆœìœ„ê°€ ë†’ì€ ìˆœì„œë¡œ ì •ë ¬

âš ï¸ **ì£¼ì˜ì‚¬í•­:**
- ë³´ì•ˆ ê³ ë ¤ì‚¬í•­
- ì„±ëŠ¥ ìµœì í™” í¬ì¸íŠ¸
- ì ì¬ì  ë¬¸ì œì 

í•œêµ­ì–´ë¡œ ê°œë°œìê°€ ë°”ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆë„ë¡ êµ¬ì²´ì ì´ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
        """
        
        print(f"Processing request enhancement: {user_request[:50]}...")
        
        result = await self.gemini.consult_gemini(
            query=enhancement_prompt,
            context="ìš”ì²­ ê°œì„  ë° êµ¬ì²´í™”",
            comparison_mode=False
        )
        
        if result['status'] == 'success':
            response_text = f"ğŸš€ **ìš”ì²­ ê°œì„  ì™„ë£Œ**\n\n"
            response_text += f"**ì›ë³¸ ìš”ì²­:** {user_request}\n\n"
            response_text += f"**ê°œì„ ëœ ìš”êµ¬ì‚¬í•­:**\n{result['response']}\n\n"
            response_text += f"â±ï¸ *ë¶„ì„ ì™„ë£Œ ì‹œê°„: {result['execution_time']:.2f}s*"
        else:
            response_text = f"âŒ **ìš”ì²­ ê°œì„  ì‹¤íŒ¨**\n\n{result.get('error', 'Unknown error')}"
        
        return [types.TextContent(type="text", text=response_text)]

    async def _handle_smart_code_generation(self, arguments: Dict[str, Any]) -> List[types.TextContent]:
        """ê°œì„ ëœ ìš”ì²­ìœ¼ë¡œ ë‹¨ê³„ë³„ ì½”ë“œ ìƒì„± ê°€ì´ë“œ ì œê³µ"""
        enhanced_request = arguments.get('enhanced_request', '')
        tech_stack = arguments.get('tech_stack', '')
        complexity_level = arguments.get('complexity_level', 'intermediate')
        
        if not enhanced_request:
            return [types.TextContent(
                type="text",
                text="âŒ Error: 'enhanced_request' parameter is required"
            )]
        
        # ì½”ë“œ ìƒì„± ê°€ì´ë“œë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸
        code_guide_prompt = f"""
ë‹¤ìŒ ê°œì„ ëœ ìš”êµ¬ì‚¬í•­ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¨ê³„ë³„ ì½”ë“œ ìƒì„± ê°€ì´ë“œë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.

ê°œì„ ëœ ìš”êµ¬ì‚¬í•­:
{enhanced_request}

ê¸°ìˆ  ìŠ¤íƒ: {tech_stack if tech_stack else "ë²”ìš©ì ìœ¼ë¡œ ì ìš© ê°€ëŠ¥í•œ ë°©ì‹"}
ë³µì¡ë„ ë ˆë²¨: {complexity_level}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

ğŸ—ï¸ **í”„ë¡œì íŠ¸ êµ¬ì¡°:**
```
í´ë”/íŒŒì¼ êµ¬ì¡° ì˜ˆì‹œ
```

ğŸ“¦ **í•„ìš”í•œ ì˜ì¡´ì„±:**
- ì„¤ì¹˜í•´ì•¼ í•  íŒ¨í‚¤ì§€ë“¤
- ì„¤ì • íŒŒì¼ë“¤

ğŸ’» **í•µì‹¬ ì½”ë“œ ìŠ¤ë‹ˆí«:**
ê° ì£¼ìš” ê¸°ëŠ¥ë³„ë¡œ í•µì‹¬ ì½”ë“œ ì˜ˆì‹œ ì œê³µ

ğŸ”§ **ì„¤ì • ë° í™˜ê²½:**
- í™˜ê²½ë³€ìˆ˜ ì„¤ì •
- ì´ˆê¸° ì„¤ì • ë°©ë²•

ğŸ§ª **í…ŒìŠ¤íŠ¸ ë°©ë²•:**
- ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ë°©ë²•
- ë””ë²„ê¹… íŒ

ğŸ“š **ì¶”ê°€ ë¦¬ì†ŒìŠ¤:**
- ì°¸ê³  ë¬¸ì„œ
- ìœ ìš©í•œ íŠœí† ë¦¬ì–¼

í•œêµ­ì–´ë¡œ ê°œë°œìê°€ ë°”ë¡œ ë”°ë¼í•  ìˆ˜ ìˆë„ë¡ êµ¬ì²´ì ì¸ ì½”ë“œì™€ ëª…ë ¹ì–´ë¥¼ í¬í•¨í•´ì„œ ì‘ì„±í•´ì£¼ì„¸ìš”.
        """
        
        print(f"Generating code guide for complexity level: {complexity_level}")
        
        result = await self.gemini.consult_gemini(
            query=code_guide_prompt,
            context="ì½”ë“œ ìƒì„± ê°€ì´ë“œ",
            comparison_mode=False
        )
        
        if result['status'] == 'success':
            response_text = f"ğŸ’» **ìŠ¤ë§ˆíŠ¸ ì½”ë“œ ìƒì„± ê°€ì´ë“œ**\n\n"
            response_text += f"**ê¸°ìˆ  ìŠ¤íƒ:** {tech_stack if tech_stack else 'ë²”ìš©'}\n"
            response_text += f"**ë³µì¡ë„:** {complexity_level}\n\n"
            response_text += result['response']
            response_text += f"\n\nâ±ï¸ *ê°€ì´ë“œ ìƒì„± ì‹œê°„: {result['execution_time']:.2f}s*"
        else:
            response_text = f"âŒ **ì½”ë“œ ê°€ì´ë“œ ìƒì„± ì‹¤íŒ¨**\n\n{result.get('error', 'Unknown error')}"
        
        return [types.TextContent(type="text", text=response_text)]

    async def _handle_enhance_user_request(self, arguments: Dict[str, Any]) -> List[types.TextContent]:
        """ì‚¬ìš©ì ìš”ì²­ì„ í•œ ë²ˆì— ë¶„ì„í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ê°œë°œ ê³„íšìœ¼ë¡œ ë³€í™˜"""
        user_request = arguments.get('user_request', '')
        project_info = arguments.get('project_info', '')
        output_format = arguments.get('output_format', 'detailed_plan')
        
        if not user_request:
            return [types.TextContent(
                type="text",
                text="âŒ Error: 'user_request' parameter is required"
            )]
        
        # ì¶œë ¥ í˜•ì‹ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ì¡°ì •
        format_instructions = {
            'detailed_plan': "ìƒì„¸í•œ ê°œë°œ ê³„íšê³¼ ë‹¨ê³„ë³„ êµ¬í˜„ ê°€ì´ë“œë¥¼ ì œê³µ",
            'quick_guide': "ë¹ ë¥¸ êµ¬í˜„ì„ ìœ„í•œ í•µì‹¬ í¬ì¸íŠ¸ë§Œ ê°„ë‹¨íˆ ì œê³µ",
            'step_by_step': "ë‹¨ê³„ë³„ë¡œ ë”°ë¼í•  ìˆ˜ ìˆëŠ” ì‹¤í–‰ ê°€ì´ë“œ ì œê³µ"
        }
        
        comprehensive_prompt = f"""
ì‚¬ìš©ìì˜ ê°œë°œ ìš”ì²­ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  ë°”ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•œ ê°œë°œ ê³„íšìœ¼ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ìš”ì²­: "{user_request}"
í”„ë¡œì íŠ¸ ì •ë³´: {project_info if project_info else "ì •ë³´ ì—†ìŒ"}
ì¶œë ¥ í˜•ì‹: {format_instructions.get(output_format, format_instructions['detailed_plan'])}

ë‹¤ìŒ êµ¬ì¡°ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

ğŸ¯ **ìš”ì²­ ë¶„ì„ ë° ëª©í‘œ:**
- ì‚¬ìš©ìê°€ ì›í•˜ëŠ” í•µì‹¬ ê¸°ëŠ¥
- ì˜ˆìƒë˜ëŠ” ê¸°ìˆ ì  ìš”êµ¬ì‚¬í•­

ğŸ“‹ **êµ¬ì²´ì  êµ¬í˜„ ê³„íš:**
- í•„ìš”í•œ ê¸°ëŠ¥ë“¤ì„ ì„¸ë¶„í™”
- ìš°ì„ ìˆœìœ„ë³„ ì •ë ¬
- ê° ê¸°ëŠ¥ì˜ ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­

ğŸ› ï¸ **ê¸°ìˆ  ìŠ¤íƒ ë° ë„êµ¬:**
- ê¶Œì¥ í”„ë ˆì„ì›Œí¬/ë¼ì´ë¸ŒëŸ¬ë¦¬
- ê°œë°œ ë„êµ¬ ë° í™˜ê²½ ì„¤ì •

ğŸ“ **ë‹¨ê³„ë³„ ì‹¤í–‰ ê°€ì´ë“œ:**
1. í™˜ê²½ ì„¤ì • ë° ì´ˆê¸° ì„¤ì •
2. í•µì‹¬ ê¸°ëŠ¥ êµ¬í˜„ ìˆœì„œ
3. í…ŒìŠ¤íŠ¸ ë° ë°°í¬ ì¤€ë¹„

ğŸ’¡ **í•µì‹¬ ì½”ë“œ ìŠ¤ë‹ˆí«:**
- ì£¼ìš” ê¸°ëŠ¥ë³„ ì½”ë“œ ì˜ˆì‹œ
- ì„¤ì • íŒŒì¼ ì˜ˆì‹œ

âš ï¸ **ì£¼ì˜ì‚¬í•­ ë° íŒ:**
- ë³´ì•ˆ ê³ ë ¤ì‚¬í•­
- ì„±ëŠ¥ ìµœì í™”
- ì¼ë°˜ì ì¸ ì‹¤ìˆ˜ ë°©ì§€

ğŸ”— **ë‹¤ìŒ ë‹¨ê³„:**
- êµ¬í˜„ í›„ í™•ì¥ ê°€ëŠ¥í•œ ê¸°ëŠ¥ë“¤
- ì¶”ê°€ í•™ìŠµ ë¦¬ì†ŒìŠ¤

í•œêµ­ì–´ë¡œ ê°œë°œìê°€ ë°”ë¡œ ì‹œì‘í•  ìˆ˜ ìˆë„ë¡ ì‹¤ìš©ì ì´ê³  êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
        """
        
        print(f"Processing comprehensive request enhancement: {user_request[:50]}...")
        
        result = await self.gemini.consult_gemini(
            query=comprehensive_prompt,
            context="ì¢…í•©ì  ìš”ì²­ ë¶„ì„ ë° ê°œë°œ ê³„íš",
            comparison_mode=False
        )
        
        if result['status'] == 'success':
            response_text = f"ğŸš€ **ì¢…í•© ê°œë°œ ê³„íš ì™„ë£Œ**\n\n"
            response_text += f"**ì›ë³¸ ìš”ì²­:** {user_request}\n"
            response_text += f"**ì¶œë ¥ í˜•ì‹:** {output_format}\n\n"
            response_text += result['response']
            response_text += f"\n\nâ±ï¸ *ê³„íš ìˆ˜ë¦½ ì‹œê°„: {result['execution_time']:.2f}s*"
            response_text += f"\nğŸ’¡ *ì´ì œ ì´ ê³„íšì„ ë°”íƒ•ìœ¼ë¡œ AI ì½”ë”© ë„êµ¬ì—ê²Œ êµ¬ì²´ì ì¸ êµ¬í˜„ì„ ìš”ì²­í•˜ì„¸ìš”!*"
        else:
            response_text = f"âŒ **ê°œë°œ ê³„íš ìˆ˜ë¦½ ì‹¤íŒ¨**\n\n{result.get('error', 'Unknown error')}"
        
        return [types.TextContent(type="text", text=response_text)]

    async def run(self):
        """Run the MCP server"""
        print("Starting MCP server...")
        async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):
            await self.server.run(
                read_stream,
                write_stream,
                self.server.create_initialization_options()
            )


async def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="MCP Server with Gemini Integration")
    parser.add_argument("--project-root", type=str, default=".", 
                       help="Project root directory (default: current directory)")
    parser.add_argument("--debug", action="store_true", 
                       help="Enable debug logging")
    
    args = parser.parse_args()
    
    if args.debug:
        import logging
        logging.getLogger().setLevel(logging.DEBUG)
        print("Debug logging enabled")
    
    try:
        server = MCPServer(project_root=args.project_root)
        await server.run()
    except KeyboardInterrupt:
        print("\nServer stopped by user")
    except Exception as e:
        print(f"Server error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())